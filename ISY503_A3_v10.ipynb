{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ISY503_A3_v10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakshamnanda/ISY503-Assessment-3/blob/main/ISY503_A3_v10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libs\n",
        "\n",
        "!pip install anvil-uplink\n",
        "import anvil.server\n",
        "#connect the notebook withUplink script.\n",
        "anvil.server.connect(\"FOYNAAKTKIEMX5ZYXBECJTFE-5MSVGBRZOX2ZH4PP\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "\n",
        "!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "spell = Speller(lang='en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "Yc8lqRAOsyOL",
        "outputId": "34817305-3337-42f8-e3a2-89bc45ecd37a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anvil-uplink in /usr/local/lib/python3.7/dist-packages (0.3.42)\n",
            "Requirement already satisfied: ws4py in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (0.5.1)\n",
            "Collecting argparse\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (0.16.0)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.7/dist-packages (2.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read XML review files\n",
        "\n",
        "positive_reviews = BeautifulSoup(open('/content/positive1.review').read(), features=\"html5lib\")\n",
        "positive_reviews = positive_reviews.findAll('review_text')\n",
        "positive_reviews = [i.contents[0] for i in positive_reviews]\n",
        "\n",
        "negative_reviews = BeautifulSoup(open('/content/negative1.review').read(), features=\"html5lib\")\n",
        "negative_reviews = negative_reviews.findAll('review_text')\n",
        "negative_reviews = [i.contents[0] for i in negative_reviews]"
      ],
      "metadata": {
        "id": "28f-6_-qs2Dk"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frame it\n",
        "\n",
        "data = {'Text': positive_reviews + negative_reviews, 'Sentiment': [1] * len(positive_reviews) + [0] * len(negative_reviews)}\n",
        "df = pd.DataFrame(data = data)"
      ],
      "metadata": {
        "id": "tUWqe1vz_dY7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tests variables\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Sentiment'], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "zGzxVZGQtbBj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "english_stopwords = stopwords.words('english')\n",
        "\n",
        "def isMisspelled(word):\n",
        "  return word not in words.words()\n",
        "\n",
        "def clear(sen, fix_misspelling = False):\n",
        "    sen = sen.lower() # set all to downcase\n",
        "    tokens = nltk.tokenize.word_tokenize(sen) # split string into tokens\n",
        "    tokens = [t for t in tokens if len(t) >= 3] # remove words shorter than 3 chracters\n",
        "    tokens = [t.translate(str.maketrans('', '', string.punctuation)) for t in tokens] # remove punctuation\n",
        "    tokens = [t for t in tokens if not any(i.isdigit() for i in t)] # remove digits\n",
        "    if fix_misspelling: # default false, for performance purposes\n",
        "      tokens = [spell(t) for t in tokens if isMisspelled(t)] # check/fix misspelling\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
        "    tokens = [t for t in tokens if t not in english_stopwords] # remove stopwords    \n",
        "    return tokens\n",
        "\n",
        "# encode into numerical\n",
        "count_vect_model = CountVectorizer(tokenizer = clear)\n",
        "count_vect_model.fit(X_train)\n",
        "\n",
        "X_train_cnt = count_vect_model.transform(X_train)\n",
        "X_test_cnt = count_vect_model.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train_cnt, y_train)\n",
        "\n",
        "print(\"Test Accuracy:\", round(model.score(X_test_cnt, y_test), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_Az4HmUuGhh",
        "outputId": "7ca948a0-6377-441e-bd99-b4ee44e17fb6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auxiliar class to compare future entries\n",
        "class ValuedWord:\n",
        "  def __init__(self, word, value):\n",
        "    self.word = word\n",
        "    self.value = value\n",
        "\n",
        "valuedWords = []\n",
        "\n",
        "# Examinating of the words that have the biggest impact on prediction\n",
        "threshold = 0.8\n",
        "for i, j in zip(count_vect_model.get_feature_names_out(), model.coef_[0]):\n",
        "    if j > threshold or j < -threshold:\n",
        "        vw = ValuedWord(i, round(j, 3))\n",
        "        valuedWords.append(vw)"
      ],
      "metadata": {
        "id": "QrxuK9LayPMi"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Call the function to anvil \n",
        "@anvil.server.callable\n",
        "def entryText(entrySen):\n",
        "  # Splitting sentence into cleaned tokens\n",
        "  entered_words = clear(entrySen, False)\n",
        "\n",
        "  senValue = 0\n",
        "  # Compare each word from the entered sentence with the those from the XML  \n",
        "  for vw in valuedWords:\n",
        "    for word in entered_words:\n",
        "      if word == vw.word:\n",
        "        print(vw.word, vw.value)\n",
        "        senValue += vw.value\n",
        "\n",
        "  # Sum up sentiments value\n",
        "  if senValue != 0:\n",
        "    if senValue > 0:\n",
        "      return 'The sentence is positive!'\n",
        "      \n",
        "    else:\n",
        "      return'The sentence is negative!'\n",
        "     \n"
      ],
      "metadata": {
        "id": "Mdb8RiSKm_pA"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keeps the notebook running and  the app calls the functions indefinitely.\n",
        "anvil.server.wait_forever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1mNQ-xbniOa",
        "outputId": "f25410dd-8ad0-44f6-f3a5-b7e6465f5120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love 1.178\n",
            "love 1.178\n",
            "bad -0.985\n",
            "love 1.178\n"
          ]
        }
      ]
    }
  ]
}